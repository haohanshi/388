{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level of Education Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alex Shi, Mark Lee, Jun Ma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is based on the idea of predicting education level by observing user behavior. More specifically, we plan to analyze the public comments of users on online forums and social media, including Facebook, CollegeConfidential, and Reddit, use natural language processing to estimate the level of sophistication of said comments, and correlate the estimations with the actual education level of the users.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Generally, we have created a model that accepts as input a \"comment\" (some body of English text), and classifies the comment within one of three education levels: \n",
    "\n",
    "- college or above (2)\n",
    "- high school (1)\n",
    "- below high school (0)\n",
    "\n",
    "One could imagine extending this model by creating an interface that takes as input a user's Facebook page, scrapes the user's public comments, and uses the model to make predictions about the user's education level.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "Our initial idea was to directly scrape comments and education levels from users' Facebook pages using the [Graph API](https://developers.facebook.com/docs/graph-api). However, we found that it was not only difficult to find many users who reveal their education publicly on their profiles, but also that the API prevents us from directly retrieving timeline posts from a user unless he or she explicitly grants us [permission](https://developers.facebook.com/docs/graph-api/reference/v2.5/user/feed) to do so.\n",
    "\n",
    "We therefore modified our strategy to search directly for people between a certain age range, and then scrape comments from the resulting page. For instance, the following page:\n",
    "> https://www.facebook.com/search/20/30/users-age-2\n",
    "\n",
    "Consists of a list of users who are between 20 and 30 years old. \n",
    "\n",
    "However, this approach has its own shortcomings. For one, building a robust parser with BeautifulSoup was non-trivial, given the complicated and widely varying structure of Facebook pages. More importantly, this approach does not solve the problem of users not revealing their education levels or comments publicly, as well as returns results for users who aren't very far removed from our existing social circles. As a result, we weren't able to scrape that many comments, and even for comments were able to scrape, we found quite low variance in terms of features.\n",
    "\n",
    "Rather than seeking out users directly, we decided to target specific demographics by pulling data from different online groups. For instance, rather than trying to find users who publicly reveal that they have no high school education, we found pages whose audience is known to primarily consist of younger, less educated audiences. Of course, we make the (potentially questionable) assumption that comments on those pages are representative of an average comment within that education level. To attempt to verify the legitimacy of this approach, we used cross-validation within each online group, as well as across different unrelated groups to measure accuracy (the results of which will be presented later). \n",
    "\n",
    "**The following summarizes our choice of groups for training:**\n",
    "\n",
    "- College or above:\n",
    "    - [The New Yorker](https://www.facebook.com/newyorker)\n",
    "    - [The New York Times](https://www.facebook.com/nytimes)\n",
    "    - [IEEE](https://www.facebook.com/IEEE.org/?fref=ts)\n",
    "    - [Psychology](https://www.facebook.com/elsevierpsychology/)\n",
    "    - [Facebook Engineering](https://www.facebook.com/engineering/)\n",
    "    - [Nature](https://www.facebook.com/nature/)\n",
    "- High school:\n",
    "    - [Justin Bieber](https://www.facebook.com/justin.bieber.film)\n",
    "    - [Twilight](https://www.facebook.com/TwilightMovie)\n",
    "    - [College Confidential Discussion Board](https://talk.collegeconfidential.com)\n",
    "    - [Reddit Debate Forum](https://www.reddit.com/r/Debate/)\n",
    "    - [Worldstar Hip Hop](https://www.facebook.com/worldstarhiphop)\n",
    "- Below high school:\n",
    "    - [Club Penguin](https://www.facebook.com/clubpenguin)\n",
    "    - [Minecraft](https://www.facebook.com/minecraft)\n",
    "    - [Gucci Mane](https://www.facebook.com/guccimane)\n",
    "    \n",
    "**The following are for testing:**\n",
    "\n",
    "- College or above:\n",
    "    - [The Economist](https://www.facebook.com/TheEconomist)\n",
    "- High school:\n",
    "    - [HipHopDX](https://www.facebook.com/HipHopDX/?fref=ts)\n",
    "- Below high school:\n",
    "    - [Desiigner](https://www.facebook.com/LifeOfDesiigner/)\n",
    "    \n",
    "__*Reasons for chooing those above pages and forums:*__\n",
    "\n",
    "The reason why we choose these facebook pages for college or above education level is that all of these pages are related to college-level research topics: Engineering, Psychology, Economy etc. Therefore, the people who post comments on these pages are mainly college students or scholars. In order to make our dataset more diverse, we add New York Times and New Yorker that contains non-academic discussions.\n",
    "\n",
    "For high school level texts, we collected our first set of data based on the assumption that certain pages are teenager-oriented. We investigated pages and websites that are popular among high school students e.g. Justin Bieber and Twillight fan pages. However, these data are lopsided because the contents usually focus on one particular topic. For example, most of the comments scraped from Justin Bieber fan page are basically all about the star, instead of representing high school students in general. Therefore, we expanded our scope to increase the diversity of our sources. The most relevent source we could find was the College Confidential Discussion Board in SAT/ACT preparation and Reddit Debate forum because first of all, they contain high school users exclusively. Also they contain more diverse topics that high school students can possibly talk about.\n",
    "\n",
    "Lastly, we looked for several kinds of data that can contains features of below highschool level language. We made our choice mainly by investigating sites that contain young audiences or sites where people post low-level English(e.g. meaningless, short, and phrases spelling mistakes). And by listing the features of all the data we collected we found that our data accurately fit our assumption.\n",
    "\n",
    "## Scrapers\n",
    "\n",
    "**For Facebook pages**\n",
    "\n",
    "We use the Facebook Graph API, which is basically the same as using HTTP requests with additional access token and unique page ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These functions are used for scraping comments from public \n",
    "# facebook pages.\n",
    "# The facebook graph api is used for getting access to the \n",
    "# comments from public pages.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "# access token for facebook graph api\n",
    "app_id = \"1786012341650556\"\n",
    "app_secret = \"deaada8ad48ddb190897068758c4d0ae\"\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "####################### helper functions #######################\n",
    "\n",
    "def getid(pagename):\n",
    "    url = (\"https://graph.facebook.com/\" + \n",
    "            pagename + \n",
    "           \"?access_token=\" + \n",
    "            access_token)\n",
    "    text = str(requests.get(url).text)\n",
    "    index = text.find('id\":\"')\n",
    "    truncated_id = text[index + 5 : -2]\n",
    "    return truncated_id\n",
    "\n",
    "def getpost(pagename):\n",
    "    msg = []\n",
    "    id_list = []\n",
    "    page_id = getid(pagename)\n",
    "\n",
    "    url = (\"https://graph.facebook.com/v2.8/\" + \n",
    "            page_id + \n",
    "           \"/posts/?fields=message&limit=100&access_token=\" + \n",
    "            access_token)\n",
    "\n",
    "    text = requests.get(url).text\n",
    "    data = json.loads(text, strict=False)\n",
    "\n",
    "    for set in data[\"data\"]:\n",
    "        if \"message\" in set:\n",
    "            msg.append(set[\"message\"])\n",
    "        if \"id\" in set:\n",
    "            id_list.append(set[\"id\"])\n",
    "\n",
    "    return msg, id_list\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "# main function for getting comments from the given page\n",
    "def getcomments(pagename):\n",
    "    id_list = getpost(pagename)[1]\n",
    "\n",
    "    raw = [] # comments in dict format\n",
    "    parsed = [] # comments parsed to text only format\n",
    "\n",
    "    num_comments = 0\n",
    "    while num_comments < 10:\n",
    "        for page_id in id_list:\n",
    "\n",
    "            url = (\"https://graph.facebook.com/v2.8/\" + \n",
    "                    page_id + \n",
    "                   \"/comments?access_token=\" + \n",
    "                    access_token)\n",
    "\n",
    "            response = (requests.get(url).text)\n",
    "            raw_comments = {}\n",
    "            parsed_comments = []\n",
    "\n",
    "            try:\n",
    "                raw_comments = json.loads(response, strict=False)[\"data\"]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for comment in raw_comments:\n",
    "                try:\n",
    "                    comment = comment[\"message\"].encode(\"ascii\")\n",
    "                    comment = comment.decode(\"ascii\")\n",
    "\n",
    "                    # get sentences with more than 5 words\n",
    "                    if (len(comment.split(\" \")) > 5):\n",
    "                        num_comments += 1\n",
    "                        parsed_comments.append(comment)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            raw.append(raw_comments)\n",
    "            parsed.append(parsed_comments)\n",
    "            # preventing \n",
    "            time.sleep(0.2)\n",
    "        \n",
    "    return raw, parsed\n",
    "\n",
    "\n",
    "# list of page names for scraping comments\n",
    "pages = [\n",
    "'nytimes',\n",
    "'newyorker',\n",
    "'TheEconomist',\n",
    "'justin.bieber.film',\n",
    "'TwilightMovie',\n",
    "'minecraft',\n",
    "'clubpenguin',\n",
    "'nature',\n",
    "'engineering',\n",
    "'elsevierpsychology',\n",
    "'IEEE.org',\n",
    "'guccimane',\n",
    "'HipHopDX_70',\n",
    "'LifeofDesiigner',\n",
    "'worldstarhiphop'\n",
    "]\n",
    "\n",
    "# scrape data and write into json files\n",
    "for page in pages:\n",
    "    print(\"getting data for {} ...\".format(page))\n",
    "    raw, parsed = getcomments(page)\n",
    "    print(\"writing raw data ...\")\n",
    "    with open('{}_raw.json'.format(page), 'w') as outfile:\n",
    "        json.dump(raw, outfile)\n",
    "\n",
    "    print(\"finish writing raw data from {}\".format(page))\n",
    "    print(\"writing comments ...\")\n",
    "    with open('{}.json'.format(page), 'w') as outfile:\n",
    "        json.dump(parsed, outfile)\n",
    "    print(\"finish writing comments from {}\".format(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**College Confidential and Reddit**\n",
    "\n",
    "We scrape the webpages using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) since they are structurally simpler than Facebook pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Reddit comments\n",
    "def getreddit(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    set = soup.findAll(\"p\")\n",
    "    count = 0\n",
    "    result = []\n",
    "    for i in set:\n",
    "        if i.attrs == {}:\n",
    "            if i.string != None:\n",
    "                if len(i.string) > 25:\n",
    "                    count += 1\n",
    "                    if count > 10:\n",
    "                        result.append(i.string)\n",
    "    return result\n",
    "\n",
    "# get College Confidential posts\n",
    "def getcomment(turl, index):\n",
    "    comment = []\n",
    "    for i in range(1, index + 1):\n",
    "        print i\n",
    "        if i == 1:\n",
    "            url = turl + \".html\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            set = soup.findAll(\"div\", class_=\"Message\")\n",
    "            for element in set:\n",
    "                comment.append(string.strip(element.contents[0]))\n",
    "        else:\n",
    "            p = \"-p\" + str(i)\n",
    "            url = turl + p + \".html\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            set = soup.findAll(\"div\", class_=\"Message\")\n",
    "            for element in set:\n",
    "                comment.append(string.strip(element.contents[0]))\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Test set selection:*__\n",
    "\n",
    "For each educational category, we choose one page for testing and validating our training results. From the above college(2) category, we chose [The Economist](https://www.facebook.com/TheEconomist), from the high school(1) category, we chose [HipHopDX.com](https://www.facebook.com/HipHopDX/?fref=ts), and from the below high school(0) category, we chose [Desiigner](https://www.facebook.com/LifeOfDesiigner/). According to our inspection of the data we found that the comments from these three pages are representatitive of the group they are in and we will use them to validate our prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training\n",
    "\n",
    "For each training data set (i.e. not \"The Economist\", \"HipHopDX\", or \"Desiigner\", which are used for testing), we run several pre-processing steps:\n",
    "\n",
    "\n",
    "1. Convert each comment into a vector of features (see below)\n",
    "2. Associate each vector of features with a label (0, 1, or 2, as specified in previous section)\n",
    "3. Use 70% of the labeled examples for training, and reserve 30% for the holdout validation set\n",
    "4. Fit the model with the 70% training set, and then evaluate performance on the 30% validation set\n",
    "\n",
    "**Features**\n",
    "\n",
    "1. Ignore short comments (less than 3 words)\n",
    "2. Ignore non-English comments (using the [langid](https://github.com/saffsd/langid.py) library)\n",
    "3. Filter out punctuation (including things like emojis), proper nouns, and links\n",
    "4. Compute a vector of four metrics:\n",
    "    - `syllables_per_word`: count the total number of syllables and divide by total number of words\n",
    "    - `words_per_sentence`: count the total number of words and divide by total number of sentences\n",
    "    - `spelling_errors_per_sentence`: count the total number of spelling errors and divide by total number of sentences\n",
    "    - `grammer_errors_per_sentence`: count the total number of grammar errors and divide by total number of sentences\n",
    "\n",
    "To compute the syllables per word, we use the syllable counter in [nltk_contrib](https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/readability/syllables_en.py). To identify spelling errors, we check whether words (proper nouns i.e capitalized words excluded) are within a [dictionary of english words](https://github.com/dwyl/english-words). To identify grammer errors, we use the [grammer-check](https://pypi.python.org/pypi/grammar-check) library, which gives us the number of total grammar errors.\n",
    "\n",
    "The relevant feature generation code is listed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert comment into sentences\n",
    "def get_sentences (doc):\n",
    "    return sent_tokenize(doc)\n",
    "\n",
    "# convert sentence into words\n",
    "def get_words (sentence):\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    words = []\n",
    "    for token in tokenizer.tokenize(sentence):\n",
    "        token = token.decode(\"utf-8\")\n",
    "\n",
    "        # remove urls\n",
    "        modified = re.sub(url_regex, '', token)\n",
    "        # remove punctuation\n",
    "        modified = modified.translate(punctuation_table)\n",
    "        # remove proper nouns\n",
    "        modified = re.sub(proper_noun_regex, '', modified)\n",
    "        # remove whitespace and standalone numbers\n",
    "        modified = re.sub(space_or_num_regex, '', modified)\n",
    "\n",
    "        if len(modified):\n",
    "            words.append(token) \n",
    "\n",
    "    return words\n",
    "\n",
    "# compute for a specific comment:\n",
    "# number of syllables, number of words, number of spelling errors,\n",
    "# number of grammar errors, number of sentences\n",
    "def get_metrics (doc):\n",
    "    global grammar_tool\n",
    "\n",
    "    # initialize dict\n",
    "    metrics = [\n",
    "        'syllables', 'words', 'spelling_errors', 'grammar_errors', 'sentences'\n",
    "    ]\n",
    "    res = { metric: 0 for metric in metrics }\n",
    "\n",
    "    # initial parse\n",
    "    sentences = get_sentences(doc)\n",
    "\n",
    "    # get metrics\n",
    "    num_sentences = len(sentences)\n",
    "    res['sentences'] = num_sentences\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            try:\n",
    "                res['grammar_errors'] += len(grammar_tool.check(sentence))\n",
    "            except Exception as e:\n",
    "                print \"grammar tool failed: {}\".format(e)\n",
    "                print \"reinitializing grammar tool..\"\n",
    "                grammar_tool = grammar_check.LanguageTool('en-US')\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            words_for_sentence = get_words(sentence)\n",
    "            res['words'] += len(words_for_sentence)\n",
    "\n",
    "            for word in words_for_sentence:\n",
    "                try:\n",
    "                    # handle trailing punctuation for spellchecker\n",
    "                    if word[-1] in string.punctuation:\n",
    "                        word = word[:-1]\n",
    "                    res['syllables'] += count_syllables(word)\n",
    "                    if not spelling_tool.check(word):\n",
    "                        res['spelling_errors'] += 1\n",
    "                except Exception as e:\n",
    "                    print \"inner exception:\", e\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print \"outer exception:\", e\n",
    "            continue\n",
    "\n",
    "    if res['words'] == 0:\n",
    "        print \"discarding...\", doc\n",
    "\n",
    "    return res\n",
    "\n",
    "# given a set of metrics returned by get_metrics, compute the feature vector:\n",
    "# `syllables_per_word`: count the total number of syllables and divide by\n",
    "# total number of words\n",
    "# `words_per_sentence`: count the total number of words and divide by total\n",
    "# number of sentences\n",
    "# `spelling_errors_per_sentence`: count the total number of spelling\n",
    "# errors and divide by total number of sentences\n",
    "# `grammer_errors_per_sentence`: count the total number of\n",
    "# grammer errors and divide by total number of sentences\n",
    "def get_features (metrics):\n",
    "    num_sentences = metrics['sentences']\n",
    "\n",
    "    # document is too short\n",
    "    if (num_sentences == 0 or metrics['words'] < MIN_WORDS_PER_DOC):\n",
    "        return None\n",
    "\n",
    "    res = []\n",
    "    num_sentences = float(num_sentences)\n",
    "\n",
    "    # compute features\n",
    "    res.append(metrics['syllables'] / float(metrics['words']))\n",
    "    res.append(metrics['words'] / num_sentences)\n",
    "    res.append(metrics['spelling_errors'] / num_sentences)\n",
    "    res.append(metrics['grammar_errors'] / num_sentences)\n",
    "    return np.array(res)\n",
    "\n",
    "# given a list of docs (body of text), parse into tokens\n",
    "# if doc is too short, skip\n",
    "# otherwise, use the tokens to build a feature, and label appropriately\n",
    "def create_features (docs, labels):\n",
    "    X, y = [], []\n",
    "    non_english = 0\n",
    "    too_short = 0\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        # ignore if not english\n",
    "        if langid.classify(doc)[0] != 'en':\n",
    "            non_english += 1\n",
    "            continue\n",
    "\n",
    "        metrics = get_metrics(doc)\n",
    "        features = get_features(metrics)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(labels[i])\n",
    "        else:\n",
    "            too_short += 1\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print X.shape, y.shape, non_english, too_short\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "Initially, we used `sklearn.svm.SVC` as our model, using grid search to optimize the regularization parameter and scoring the accuracy over various kernels to determine the optimal kernel. However, we found that even when using the most accurate kernel (\"poly\"), the best performance we were able to achieve was 45% accuracy on the validation set. Furthermore, the non-linear kernel took several minutes just to train on the data, which prevented us from doing more extensive grid search. We fell back on the \"linear\" kernel which performed relatively similarly to \"poly\" (and also ran much faster), but in the end optimizing the regularization only resulted in 47% accuracy on the validation set.\n",
    "\n",
    "The next thing we tried was LogisticRegression, which was comparable to the SVM but ran much faster (on the order of seconds, as opposed to minutes), and yet seemed to have comparable support for multi-class classification as SVM with non-linear kernel. Indeed, using just the default parameters we were able to achieve close to 45% accuracy, and because of the speed, we were able to perform a much more thorough search over the hyperparameters. When running with the \"lbfgs\" solver, with regularization set to 10 and max iterations to 100, we were able to achieve close to 49% accuracy with much less training time.\n",
    "\n",
    "Rather than using grid search, we also tried using LogisticRegressionCV, which performs kfold cross-validation in order to optimize the hyperparameters. Indeed, using LogisticRegressionCV with 10 folds, regularizations from [10^-10, 10^10], and 10000 max iterations, we were able to get over 56% accuracy, which is a significant improvement.\n",
    "\n",
    "The relevant code is listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comments should be a nx1 list of strings\n",
    "# labels should be a nx1 list of ints\n",
    "# the ith label should correspond to the ith comment\n",
    "def learn_classifier (X_train, y_train, kernel='best'):\n",
    "    print \"learning classifier...\"\n",
    "    clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10))),\n",
    "        penalty='l2',\n",
    "        cv=10, # kfolds with k=10\n",
    "        random_state=42,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        solver='lbfgs',\n",
    "        tol=1e-4\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"done learning classifier\"\n",
    "    return clf\n",
    "\n",
    "# chooses optimal kernel for svm\n",
    "def optimal_svm_kernel (X_train, y_train, X_validate, y_validate):\n",
    "    best_kernel = None\n",
    "    best_accuracy = 0\n",
    "    for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "        classifier = SVM(X_train, y_train, kernel)\n",
    "        accuracy = classifier.score(X_validate, y_validate)\n",
    "        if best_kernel is None or accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_kernel = kernel\n",
    "        print kernel, \":\", accuracy\n",
    "    return best_kernel, best_accuracy\n",
    "\n",
    "# choose optimal params for logistic regression\n",
    "def optimal_hyperparams (X_train, y_train):\n",
    "    print \"running grid search...\"\n",
    "    clf = LogisticRegression()\n",
    "    params = {\n",
    "        \"solver\": [\"lbfgs\", \"newton-cg\", \"sag\"],\n",
    "        \"max_iter\": [100, 1000, 10000],\n",
    "        \"multi_class\": [\"ovr\", \"multinomial\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "    classifier = GridSearchCV(clf, params)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    best = classifier.best_params_\n",
    "    print \"best params:\", best\n",
    "    print \"best score:\", classifier.best_score_\n",
    "    print \"dumping results to file: {}...\".format(GRID_SEARCH_RESULTS_FILE)\n",
    "    with open(GRID_SEARCH_RESULTS_FILE, 'w') as f:\n",
    "        f.write(\"{}\".format(best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and analysis\n",
    "\n",
    "When running on the testing data, which our model was never trained on, we get just under 50% accuracy. While this may not seem very high, we are predicting across three categories that are, in some sense, not very discrete. Not only was the assignment of sources to categories based on our own judgment (and what limited demographic data we have online), but also the comments within and across the pages obviously are not uniformly of the same genre, tone, or style of writing. \n",
    "\n",
    "To improve our accuracy, we could try identifying different features that are perhaps more salient or representative of each education group, although our features are already relatively less biased than, say, any bag of words approach, which is even more heavily influenced by the genre of writing. \n",
    "\n",
    "For example, we visualized the feature data and observed that our expectations matched relatively well with our features. Namely, we can see that grammar and spelling errors increase as we go down the education levels, and syllables per word increases as we go up the education levels.\n",
    "\n",
    "![feature_visualization](feature_visualization.png)\n",
    "\n",
    "Perhaps more importantly is that our initial assumption, that comments from pages with audiences of certain demographics accurately represent those demographics, may not be entirely accurate, at least based on the pages we chose. To further improve the accuracy of our model, a better approach would be to find more reliable data, such as reverting to our initial idea of scraping education levels from Facebook pages using Beautifulsoup. Even though with this method we would probably get less data, the fact that we can ascertain that our sample is within a certain age group gives us relatively better confidence in the reliability of the data.\n",
    "\n",
    "Further, we have only thus far tried relatively simple models, namely LogisticRegression and SVM with \"linear\" and \"poly\" kernels (with relatively un-optimized parameters). Further investigation into different types of classification models, as well as more extensive optimization of parameters would probably yield more favorable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "- https://www.quora.com/What-are-the-demographics-of-Minecraft-players\n",
    "- http://www.ibtimes.com/audience-profiles-who-actually-reads-new-york-times-watches-fox-news-other-news-publications-1451828\n",
    "- https://pypi.python.org/pypi/pylinkgrammar\n",
    "- http://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python\n",
    "- https://github.com/dwyl/english-words\n",
    "- http://jetscram.com/blog/industry-news/social-media-user-statistics-and-age-demographics-2014/\n",
    "- http://sproutsocial.com/insights/new-social-media-demographics/\n",
    "- http://venturebeat.com/2012/08/22/social-media-demographics-stats-2012/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
