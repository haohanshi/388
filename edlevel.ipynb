{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level of Education Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alex Shi, Mark Lee, Jun Ma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is based on the idea of predicting education level by observing user behavior. More specifically, we plan to analyze the public comments of users on online forums and social media, including Facebook, CollegeConfidential, and Reddit, use natural language processing to estimate the level of sophistication of said comments, and correlate the estimations with the actual education level of the users.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Generally, we have created a model that accepts as input a \"comment\" (some body of English text), and classifies the comment within one of three education levels: \n",
    "\n",
    "- college or above (2)\n",
    "- high school (1)\n",
    "- below high school (0)\n",
    "\n",
    "One could imagine extending this model by creating an interface that takes as input a user's Facebook page, scrapes the user's public comments, and uses the model to make predictions about the user's education level.\n",
    "\n",
    "## Procedure of Project\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "Our initial idea was to directly scrape comments and education levels from users' Facebook pages using the [Graph API](https://developers.facebook.com/docs/graph-api). However, we found that it was not only difficult to find many users who reveal their education publicly on their profiles, but also that the API prevents us from directly retrieving timeline posts from a user unless he or she explicitly grants us [permission](https://developers.facebook.com/docs/graph-api/reference/v2.5/user/feed) to do so.\n",
    "\n",
    "We therefore modified our strategy to search directly for people between a certain age range, and then scrape comments from the resulting page. For instance, the following page:\n",
    "> https://www.facebook.com/search/20/30/users-age-2\n",
    "\n",
    "Consists of a list of users who are between 20 and 30 years old. \n",
    "\n",
    "However, this approach has its own shortcomings. For one, building a robust parser with BeautifulSoup was non-trivial, given the complicated and widely varying structure of Facebook pages. More importantly, this approach does not solve the problem of users not revealing their education levels or comments publicly, as well as returns results for users who aren't very far removed from our existing social circles. As a result, we weren't able to scrape that many comments, and even for comments were able to scrape, we found quite low variance in terms of features.\n",
    "\n",
    "Rather than seeking out users directly, we decided to target specific demographics by pulling data from different online groups. For instance, rather than trying to find users who publicly reveal that they have no high school education, we found pages whose audience is known to primarily consist of younger, less educated audiences. Of course, we make the (potentially questionable) assumption that comments on those pages are representative of an average comment within that education level. To show the legitimacy of this approach, we used cross-validation within each online group, as well as across different unrelated groups (the results of which will be presented later). \n",
    "\n",
    "The following summarizes our choice of groups for training:\n",
    "\n",
    "- College or above:\n",
    "    - [The New Yorker](https://www.facebook.com/newyorker)\n",
    "    - [The New York Times](https://www.facebook.com/nytimes)\n",
    "    - [IEEE](https://www.facebook.com/IEEE.org/?fref=ts)\n",
    "    - [Psychology](https://www.facebook.com/elsevierpsychology/)\n",
    "    - [Facebook Engineering](https://www.facebook.com/engineering/)\n",
    "    - [Nature](https://www.facebook.com/nature/)\n",
    "- High school:\n",
    "    - [Justin Bieber](https://www.facebook.com/justin.bieber.film)\n",
    "    - [Twilight](https://www.facebook.com/TwilightMovie)\n",
    "    - [College Confidential Discussion Board](https://talk.collegeconfidential.com)\n",
    "    - [Reddit Debate Forum](https://www.reddit.com/r/Debate/)\n",
    "    - [Worldstar Hip Hop](https://www.facebook.com/worldstarhiphop)\n",
    "- Below high school:\n",
    "    - [Club Penguin](https://www.facebook.com/clubpenguin)\n",
    "    - [Minecraft](https://www.facebook.com/minecraft)\n",
    "    - [Gucci Mane](https://www.facebook.com/guccimane)\n",
    "    \n",
    "The following are for testing:\n",
    "\n",
    "- College or above:\n",
    "    - [The Economist](https://www.facebook.com/TheEconomist)\n",
    "- High school:\n",
    "    - [HipHopDX](https://www.facebook.com/HipHopDX/?fref=ts)\n",
    "- Below high school:\n",
    "    - [Desiigner](https://www.facebook.com/LifeOfDesiigner/)\n",
    "\n",
    "__*Reasons for chooing those above pages and forums:*__\n",
    "\n",
    "__*Methods used for scraping data:*__\n",
    "\n",
    "__For Facebook pages__, we use the Facebook Graph API, which basically is the same as using HTTP requests with additional access token and unique page id returned by Facebook Graph API, finally write all the collected comments into json file, the core code looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the unique page id of a certain Facebook page\n",
    "def getid(pagename):\n",
    "    url = \"https://graph.facebook.com/\" + pagename + \"?access_token=\" + access_token\n",
    "    text = str(requests.get(url).text)\n",
    "    index = text.find('id\":\"')\n",
    "    return text[index+5:-2]\n",
    "\n",
    "# get posts and their post id\n",
    "def getpost(pagename):\n",
    "    msg = []\n",
    "    idl = []\n",
    "    id = getid(pagename)\n",
    "    url = \"https://graph.facebook.com/v2.8/\" + id + \"/posts/?fields=message&limit=100&access_token=\" + access_token\n",
    "    text = requests.get(url).text\n",
    "    data = json.loads(text, strict=False)\n",
    "    for set in data[\"data\"]:\n",
    "        if \"message\" in set:\n",
    "            msg.append(set[\"message\"])\n",
    "        if \"id\" in set:\n",
    "            idl.append(set[\"id\"])\n",
    "    return msg, idl\n",
    "\n",
    "# get comments of those posts based on post id and try to parse the comments\n",
    "def getcomments(pagename):\n",
    "    idlist = getpost(pagename)[1]\n",
    "    parsed = []\n",
    "    raw = []\n",
    "    num_comments = 0\n",
    "    while num_comments < 2000:\n",
    "        for id in idlist:\n",
    "            url = \"https://graph.facebook.com/v2.8/\" + id + \"/comments?access_token=\" + access_token\n",
    "            response = (requests.get(url).text)\n",
    "            raw_comments = {}\n",
    "            parsed_comments = []\n",
    "            try:\n",
    "                raw_comments = json.loads(response, strict=False)[\"data\"]\n",
    "            except:\n",
    "                continue\n",
    "            for comment in raw_comments:\n",
    "                try:\n",
    "                    comment = comment[\"message\"].encode(\"ascii\")\n",
    "                    comment = comment.decode(\"ascii\")            \n",
    "                    if (len(comment.split(\" \")) > 5):\n",
    "                        num_comments += 1\n",
    "                        parsed_comments.append(comment)\n",
    "                except:\n",
    "                    continue\n",
    "            raw.append(raw_comments)\n",
    "            parsed.append(parsed_comments)\n",
    "            time.sleep(0.2)\n",
    "    return raw, parsed\n",
    "\n",
    "# list of page names for scraping comments\n",
    "pages = [\n",
    "'nytimes',\n",
    "'newyorker',\n",
    "'TheEconomist',\n",
    "'justin.bieber.film',\n",
    "'TwilightMovie',\n",
    "'minecraft',\n",
    "'clubpenguin',\n",
    "'nature',\n",
    "'engineering',\n",
    "'elsevierpsychology',\n",
    "'IEEE.org',\n",
    "'guccimane',\n",
    "'HipHopDX_70',\n",
    "'LifeofDesiigner',\n",
    "'worldstarhiphop'\n",
    "]\n",
    "\n",
    "# scrape data and write into json files\n",
    "for page in pages:\n",
    "    print(\"getting data for {} ...\".format(page))\n",
    "    raw, parsed = getcomments(page)\n",
    "    print(\"writing raw data ...\")\n",
    "    with open('{}_raw.json'.format(page), 'w') as outfile:\n",
    "        json.dump(raw, outfile)\n",
    "\n",
    "    print(\"finish writing raw data from {}\".format(page))\n",
    "    print(\"writing comments ...\")\n",
    "    with open('{}.json'.format(page), 'w') as outfile:\n",
    "        json.dump(parsed, outfile)\n",
    "    print(\"finish writing comments from {}\".format(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For other two websites College Confidential and Reddit Debate Forum__, we are scraping data directly from HTML text using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) since it is much easier to get the string of text than Facebook page. The parsing method is a little different between these two websites since Reddit doesn't split one topic into different pages whereas College Confidential does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Reddit comments\n",
    "def getreddit(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    set = soup.findAll(\"p\")\n",
    "    count = 0\n",
    "    result = []\n",
    "    for i in set:\n",
    "        if i.attrs == {}:\n",
    "            if i.string != None:\n",
    "                if len(i.string) > 25:\n",
    "                    count += 1\n",
    "                    if count > 10:\n",
    "                        result.append(i.string)\n",
    "    return result\n",
    "\n",
    "# get College Confidential posts\n",
    "def getcomment(turl, index):\n",
    "    comment = []\n",
    "    for i in range(1, index + 1):\n",
    "        print i\n",
    "        if i == 1:\n",
    "            url = turl + \".html\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            set = soup.findAll(\"div\", class_=\"Message\")\n",
    "            for element in set:\n",
    "                comment.append(string.strip(element.contents[0]))\n",
    "        else:\n",
    "            p = \"-p\" + str(i)\n",
    "            url = turl + p + \".html\"\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            set = soup.findAll(\"div\", class_=\"Message\")\n",
    "            for element in set:\n",
    "                comment.append(string.strip(element.contents[0]))\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some observations:\n",
    "\n",
    "- A lot of the pages targetted towards younger audiences are mostly recreational. We virtually couldn't find any pages that were both non-recreational and had substantial posts by younger users. \n",
    "- It was very difficult to find pages for the \"below high school\" group. We suspect that kids of that age typically don't post very much online (hence most pages were dominated by parents posting on behalf of their children).\n",
    "\n",
    "As such, we will make the assumption that the above few pages are relatively representative of each education level.\n",
    "\n",
    "Having identified these pages, we then used the Facebook [Graph API](https://developers.facebook.com/docs/graph-api) to first gather all the posts, then parse all the comments within each post.\n",
    "\n",
    "## Training\n",
    "\n",
    "The end goal is to take a list of (comment, education level) tuples and train an SVM model that takes a given post or comment and outputs a predicted education level. For any given comment or post, we run the following pre-processing steps:\n",
    "\n",
    "- ignore short posts (less than 5 words)\n",
    "- ignore non-English posts (using the [langid](https://github.com/saffsd/langid.py) library)\n",
    "- filter out punctuation (including things like emojis)\n",
    "- tokenize the filtered comment or post, removing stopwords and rare words (including things like links, proper nouns)\n",
    "- compute a vector of four metrics:\n",
    "    - `syllables_per_word`: count the total number of syllables and divide by total number of words\n",
    "    - `words_per_sentence`: count the total number of words and divide by total number of sentences\n",
    "    - `spelling_errors_per_sentence`: count the total number of spelling errors and divide by total number of sentences\n",
    "    - `grammer_errors_per_sentence`: count the total number of grammer errors and divide by total number of sentences\n",
    "    \n",
    "To compute the syllables per word, we use the syllable counter in [nltk_contrib](https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/readability/syllables_en.py). To identify spelling errors, we check whether words (proper nouns i.e capitalized words excluded) are within a [dictionary of english words](https://github.com/dwyl/english-words). To identify grammer errors, we use the [grammer-check](https://pypi.python.org/pypi/grammar-check) library, which gives us the number of total grammar errors.\n",
    "\n",
    "The processed comments are then labeled based on which source it came from (e.g posts from The New Yorker will have label of 2) and then used to train the SVM using linear regression.\n",
    "\n",
    "## Predicting\n",
    "\n",
    "Given a new user's Facebook page, we use the same functions as before to scrape all posts and comments from the user, applying the same pre-processing steps as we did during training. Finally, we use the trained SVM to predict the education level.\n",
    "\n",
    "## Current status\n",
    "\n",
    "We have finished scraping all of the data and training the model. What remains is to use our model to make predictions. We are currently looking at different celebrities/prominent figures to make predictions on, as well as different ways to present our data graphically in order to capture the most interesting results.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "- https://www.quora.com/What-are-the-demographics-of-Minecraft-players\n",
    "- http://www.ibtimes.com/audience-profiles-who-actually-reads-new-york-times-watches-fox-news-other-news-publications-1451828\n",
    "- https://pypi.python.org/pypi/pylinkgrammar\n",
    "- http://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python\n",
    "- https://github.com/dwyl/english-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
