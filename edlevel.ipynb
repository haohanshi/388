{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level of Education Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alex Shi, Mark Lee, Jun Ma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is be based on the idea of predicting education level by observing user behavior. More specifically, we plan to analyze the public posts of users on social media including Facebook and BBS such as CollegeConfidential and Reddit, using natural language processing to estimate the level of sophistication of public posts, and correlating the estimations with the actual education level of the users.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Generally, we have created an interface that accepts input of a piece of text from either a person's social media or , and outputs the predicted education level of the user:\n",
    "\n",
    "- college or above (2)\n",
    "- high school (1)\n",
    "- below high school (0)\n",
    "\n",
    "The final goal is to take a user's some pieces of script and output a prediction of education level.\n",
    "\n",
    "## Procedure of Project\n",
    "### Data Collection\n",
    "At first, we aimed to directly get posts and comments of certain users from [Facebook Graph API](https://developers.facebook.com/docs/graph-api). Thus we could effectively get posts and comments from our desired group of people, e.g. high shcool students, college students etc. However, sooner we found out that it was impossible to directly get timeline posts from a person if he/she did not grant our API token [permission](https://developers.facebook.com/docs/graph-api/reference/v2.5/user/feed).\n",
    "\n",
    "Then we changed our strategy to search directly for people between a certain age range and tried to directly scrape data from HTTP request of the web page. We found out that we could search for users within ceartain age range using:\n",
    "> https://www.facebook.com/search/20/30/users-age-2\n",
    "\n",
    "This will give us users who are between 20 and 30 years old. \n",
    "\n",
    "However, when we were trying to parse the HTML text with BeautifulSoup and HTML parser, we could not get our desired data after several trails since the content structure of Facebook page is far more complicated than we predicted.\n",
    "\n",
    "Finally, after sampling users from various Facebook Pages (instead of user's profile pages) and researching on several forums and social media, we found that rather than scraping education level directly from user profiles, it would be still reliable to scrape pages with certain demographics, and assume that posts and comments on those pages are representative of an average post within that education level. The following summarizes our findings:\n",
    "\n",
    "- College or above:\n",
    "    - [The New Yorker](https://www.facebook.com/newyorker)\n",
    "    - [The Economist](https://www.facebook.com/TheEconomist)\n",
    "    - [The New York Times](https://www.facebook.com/nytimes)\n",
    "    - [IEEE](https://www.facebook.com/IEEE.org/?fref=ts)\n",
    "    - [Psychology](https://www.facebook.com/elsevierpsychology/)\n",
    "    - [Facebook Engineering](https://www.facebook.com/engineering/)\n",
    "    - [Nature](https://www.facebook.com/nature/)\n",
    "    \n",
    "- High school:\n",
    "    - [Justin Bieber](https://www.facebook.com/justin.bieber.film)\n",
    "    - [Twilight](https://www.facebook.com/TwilightMovie)\n",
    "    - [College Confidential Discussion Board](https://talk.collegeconfidential.com)\n",
    "    - [Reddit Debate Forum](https://www.reddit.com/r/Debate/)\n",
    "- Below high school:\n",
    "    - [Club Penguin](https://www.facebook.com/clubpenguin)\n",
    "    - [Minecraft](https://www.facebook.com/minecraft)\n",
    "    \n",
    "__*Reasons for chooing those above pages and forums:*__\n",
    "\n",
    "__*Methods used for scraping data:*__\n",
    "\n",
    "__For Facebook pages__, we use the Facebook Graph API, which basically is the same as using http requests with additional access token and unique page id returned by Facebook Graph API, finally write all the collected comments into jason file, the core code looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the unique page id of a certain Facebook page\n",
    "def getid(pagename):\n",
    "    url = \"https://graph.facebook.com/\" + pagename + \"?access_token=\" + access_token\n",
    "    text = str(requests.get(url).text)\n",
    "    index = text.find('id\":\"')\n",
    "    return text[index+5:-2]\n",
    "\n",
    "# get posts and their post id\n",
    "def getpost(pagename):\n",
    "    msg = []\n",
    "    idl = []\n",
    "    id = getid(pagename)\n",
    "    url = \"https://graph.facebook.com/v2.8/\" + id + \"/posts/?fields=message&limit=100&access_token=\" + access_token\n",
    "    text = requests.get(url).text\n",
    "    data = json.loads(text, strict=False)\n",
    "    for set in data[\"data\"]:\n",
    "        if \"message\" in set:\n",
    "            msg.append(set[\"message\"])\n",
    "        if \"id\" in set:\n",
    "            idl.append(set[\"id\"])\n",
    "    return msg, idl\n",
    "\n",
    "# get comments of those posts based on post id and try to parse the comments\n",
    "def getcomments(pagename):\n",
    "    idlist = getpost(pagename)[1]\n",
    "    parsed = []\n",
    "    raw = []\n",
    "    num_comments = 0\n",
    "    while num_comments < 2000:\n",
    "        for id in idlist:\n",
    "            url = \"https://graph.facebook.com/v2.8/\" + id + \"/comments?access_token=\" + access_token\n",
    "            response = (requests.get(url).text)\n",
    "            raw_comments = {}\n",
    "            parsed_comments = []\n",
    "            try:\n",
    "                raw_comments = json.loads(response, strict=False)[\"data\"]\n",
    "            except:\n",
    "                continue\n",
    "            for comment in raw_comments:\n",
    "                try:\n",
    "                    comment = comment[\"message\"].encode(\"ascii\")\n",
    "                    comment = comment.decode(\"ascii\")            \n",
    "                    if (len(comment.split(\" \")) > 5):\n",
    "                        num_comments += 1\n",
    "                        parsed_comments.append(comment)\n",
    "                except:\n",
    "                    continue\n",
    "            raw.append(raw_comments)\n",
    "            parsed.append(parsed_comments)\n",
    "            time.sleep(0.2)       \n",
    "    return raw, parsed\n",
    "\n",
    "# list of page names for scraping comments\n",
    "pages = [\n",
    "'nytimes',\n",
    "'newyorker',\n",
    "'TheEconomist',\n",
    "'justin.bieber.film',\n",
    "'TwilightMovie',\n",
    "'minecraft',\n",
    "'clubpenguin',\n",
    "'nature',\n",
    "'engineering',\n",
    "'elsevierpsychology',\n",
    "'IEEE.org'\n",
    "]\n",
    "\n",
    "# scrape data and write into json files\n",
    "for page in pages:\n",
    "    print(\"getting data for {} ...\".format(page))\n",
    "    raw, parsed = getcomments(page)\n",
    "    print(\"writing raw data ...\")\n",
    "    with open('{}_raw.json'.format(page), 'w') as outfile:\n",
    "        json.dump(raw, outfile)\n",
    "\n",
    "    print(\"finish writing raw data from {}\".format(page))\n",
    "    print(\"writing comments ...\")\n",
    "    with open('{}.json'.format(page), 'w') as outfile:\n",
    "        json.dump(parsed, outfile)\n",
    "    print(\"finish writing comments from {}\".format(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For other two websites College Confidential and Reddit Debate Forum__, we are scraping data directly from HTML text since it is much easier to get the string of text than Facebook page. The parsing method is a little different between these two websites since Reddit doesn't split one topic into different pages whereas College Confidential does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some observations:\n",
    "\n",
    "- A lot of the pages targetted towards younger audiences are mostly recreational. We virtually couldn't find any pages that were both non-recreational and had substantial posts by younger users. \n",
    "- It was very difficult to find pages for the \"below high school\" group. We suspect that kids of that age typically don't post very much online (hence most pages were dominated by parents posting on behalf of their children).\n",
    "\n",
    "As such, we will make the assumption that the above few pages are relatively representative of each education level.\n",
    "\n",
    "Having identified these pages, we then used the Facebook [Graph API](https://developers.facebook.com/docs/graph-api) to first gather all the posts, then parse all the comments within each post.\n",
    "\n",
    "## Training\n",
    "\n",
    "The end goal is to take a list of (comment, education level) tuples and train an SVM model that takes a given post or comment and outputs a predicted education level. For any given comment or post, we run the following pre-processing steps:\n",
    "\n",
    "- ignore short posts (less than 5 words)\n",
    "- ignore non-English posts (using the [langid](https://github.com/saffsd/langid.py) library)\n",
    "- filter out punctuation (including things like emojis)\n",
    "- tokenize the filtered comment or post, removing stopwords and rare words (including things like links, proper nouns)\n",
    "- compute a vector of four metrics:\n",
    "    - `syllables_per_word`: count the total number of syllables and divide by total number of words\n",
    "    - `words_per_sentence`: count the total number of words and divide by total number of sentences\n",
    "    - `spelling_errors_per_sentence`: count the total number of spelling errors and divide by total number of sentences\n",
    "    - `grammer_errors_per_sentence`: count the total number of grammer errors and divide by total number of sentences\n",
    "    \n",
    "To compute the syllables per word, we use the syllable counter in [nltk_contrib](https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/readability/syllables_en.py). To identify spelling errors, we check whether words (proper nouns i.e capitalized words excluded) are within a [dictionary of english words](https://github.com/dwyl/english-words). To identify grammer errors, we use the [grammer-check](https://pypi.python.org/pypi/grammar-check) library, which gives us the number of total grammar errors.\n",
    "\n",
    "The processed comments are then labeled based on which source it came from (e.g posts from The New Yorker will have label of 2) and then used to train the SVM using linear regression.\n",
    "\n",
    "## Predicting\n",
    "\n",
    "Given a new user's Facebook page, we use the same functions as before to scrape all posts and comments from the user, applying the same pre-processing steps as we did during training. Finally, we use the trained SVM to predict the education level.\n",
    "\n",
    "## Current status\n",
    "\n",
    "We have finished scraping all of the data and training the model. What remains is to use our model to make predictions. We are currently looking at different celebrities/prominent figures to make predictions on, as well as different ways to present our data graphically in order to capture the most interesting results.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "- https://www.quora.com/What-are-the-demographics-of-Minecraft-players\n",
    "- http://www.ibtimes.com/audience-profiles-who-actually-reads-new-york-times-watches-fox-news-other-news-publications-1451828\n",
    "- https://pypi.python.org/pypi/pylinkgrammar\n",
    "- http://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python\n",
    "- https://github.com/dwyl/english-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
